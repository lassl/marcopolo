defaults:
  - model: dpr
  - datamodule: dpr
  - trainer: dpr

# model and tokenizer configuration
model_name_or_path: roberta-base

# datamodule configuration
max_seq_length: 512
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
num_workers: 0
seed: 0

# trainer configuration
learning_rate: 3e-5
num_train_epochs: 3
gradient_accumulation_steps: 1
lr_scheduler_type: linear
logging_steps: 50

# debug
debug: False
